# TAG 
Text-to-Audio-Generator (@TextToAudioGeneratorBot)

Бот для генерации аудио по текстовому запросу.

## Bot

**Bot** – это сервис программы, в котором происходят все внешние взаимодействия с пользователем.

**context** – это модуль, в котором содержится информация о ID чата, ID пользователя, текстовый запрос о желаемой аудиозаписи и ее продолжительность в секундах. Необходим для поддержания сессии пользователя. 

**sound_request** – это модуль с такой абстракцией как request, представленной в виде класса, который в качестве атрибутов принимает ID чата, ID пользователя, текстовый запрос о желаемой аудиозаписи и ее продолжительность в секундах, а в качестве методов собирает эти данные в JSON и разбирает данные.

**sender_queue** – это модуль, в котором инкапсулируется взаимодействие с программным брокером сообщений RabbitMQ (очередью), то есть скрываются тонкости реализации взаимодействия с очередью, в том числе отправка сообщений в очередь и функция создания очереди. Этот класс упрощает взаимодействие с брокером RabbitMQ в дальнейшем, что позволяет достаточно просто отправлять сообщения в очередь. В качестве атрибута принимает уникальное имя очереди. 

**sound_responce** – это модуль, который инкапсулирует (или скрывает) тонкости по разбору и формированию JSON’а, но уже для ответного сообщения, в котором есть ID чата, ID пользователя и сгенерированная по запросу пользователя звукозапись.

**sound_receiver** – это модуль, который соединятся с брокером «ResponseQueue», далее из sound_responce она принимает сгенерированную аудиозапись, отправляет ее в Bot и вызывает метод callback, который в свою очередь очищает context, чтобы пользователь мог снова отправить запрос. sound_receiver работает в отдельном потоке, так как он (sound_receiver) запускается вместе с ботом, но из-за того, что работа бота останавливает основной поток, то эта часть запускается в отдельном потоке для возможности одновременной работы бота и приема сообщений из брокера. Другими словами, для того чтобы не случилась ситуация, когда один пользователь начал генерацию аудиозаписи, и сервис просто повис и другие пользователи не могли отправить сообщение в бота, sound_receiver работает в отдельном потоке. 

Таким образом, путь генерации аудиозаписи выглядит следующим образом: 
Bot в методе callback_worker, собрав все данные, такие как ID чата, ID пользователя, текстовый запрос о желаемой аудиозаписи и ее продолжительность в секундах и, сохранив эти данные в  context, отправляет данные о пользователе в sound_request, в котором эти данные собираются в JSON. Далее сформированный JSON отправляется в sender_queue, в очередь «RequestQueue» для дальнейшей отправки по сети интернет, в данном случае, по локальной сети, в сервис, в котором осуществляется генерация звука. 
Затем из сервиса генерации аудиозаписи sound_responce получает JSON, который содержит в себе необходимую информацию о пользователе и чате, а так же аудиозапись, разбирает этот JSON и отправляет звук в sound_receiver, который с помощью метода receive_sound отправляет аудиозапись и информацию о контексте в Bot, пользователь видит файл с аудиозаписью, и, затем метод start_after_audio_sent сообщает, что аудиозапись сгенерирована и пользователь может отправить запрос снова. 

## Generator Service

**GeneratorService** – это отдельная программа, которая запускается вместе с ботом параллельно на двух разных машинах (можно и на одной, если хватит мощностей одного компьютера) и включает в себя соответствующие модули:

**sound_generator_service** – это модуль, представляющий собой отдельный сервис, который принимает запрос из брокера «RequestQueue», генерирует по нему звук и отправляет в другой брокер «ResponseQueue». Это позволяет не нагружать Bot для того, чтобы бот оперативно отвечал на сообщения пользователей, а сервис мог отдельно генерировать звуки. Так как взаимодействие происходит через очередь, это позволяет потенциально реализовать несколько штук таких сервисов, имеется ввиду что несколько сервисов будут одновременно генерировать звук для разных пользователей. В данной реализации такой сервис один, так как есть ограничения в вычислительных мощностях, но потенциально это возможно в такой архитектуре. В архитектурах без очередей это сделать будет намного сложнее. Таким образом, такая реализация сервиса – это потенциальная возможность для его масштабирования.

**sound_request** – это модуль, который в качестве атрибутов принимает ID чата, ID пользователя, текстовый запрос о желаемой аудиозаписи и ее продолжительность в секундах, а в качестве методов собирает эти данные в JSON и разбирает данные, в данном случае он разбирает данные, полученные из брокера.

**model** – это модуль, который является генератором аудиозаписи по текстовому запросу и желаемой продолжительности в секундах. Содержит в себе сложную нейросетевую модель Make-An-Audio*, состоящую из нескольких из нейросетей. Более подробная архитектура описана ниже. 

**sound_responce** – это модуль, который скрывает тонкости по разбору и формированию JSON’а, но уже для ответного сообщения, в котором есть ID чата, ID пользователя и сгенерированная по запросу пользователя звукозапись.

**sender_queue** – это модуль, в котором, как уже было описано реанее, инкапсулируется взаимодействие с программным брокером сообщений RabbitMQ, то есть скрываются тонкости реализации взаимодействия с очередью RabbitMQ, в том числе отправка сообщений в очередь и функция создания очереди. Этот класс облегчает взаимодействие с брокером RabbitMQ в дальнейшем, что позволяет достаточно просто отправлять сообщения в очередь. В качестве атрибута принимает уникальное имя очереди. 

Таким образом, все вышеописанные компоненты GeneratorService взаимодействуют между собой следующим образом:
sound_generator_service соединяется с брокером сообщений «RequestQueue», далее он получает из «RequestQueue» все данные, а именно ID пользователя, ID чата, запрос и продолжительность в секундах, в формате JSON, помещает эти данные в sound_request, там он их разбирает, потом он отправляет запрос и продолжительность на генерацию с помощью модуля model, потом полученная звукозапись отправляется в sound_responce, там собирается в JSON и из sound_responce отправляется в брокер «ResponseQueue», и так как очередь является отдельным приложением, то по сети она отправляется в очередь, соответственно, с другой стороны, из очереди она забирается sound_receiver, который отправляет аудиозапись уже в Bot.

## Model

Для осуществления генерации была использована генеративная модель Make-An-Audio. Эта модель представляет собой эффективный метод, который применяет скрытую диффузию (latent diffusion) с автоэнкодером спектрограмм для моделирования длинных непрерывных сигналов. 

При старте сервиса «GeneratorService», компонентом которого является Model, модель инициализируется, затем в функцию sound_predict поступают на вход текстовое описание запроса и его продолжительность. Эти данные передаются в функцию predict, внутри которой определяется частота спектрограммы. Затем в sampler происходит генерация нескольких сэмплов, после чего в vocoder мел-спектрограммы преобразовываются в необработанные сигналы. Затем из нескольких сигналов выбирается наиболее подходящий. 

Модель состоит из следующих основных компонентов:  
1) Усовершенствование псевдоподсказок, позволяющее решить проблему нехватки данных, открывая возможность использования аудиофайлов без языкового содержания.  
2) Автоэнкодер спектрограмм для прогнозирования self-supervised представления вместо длинных непрерывных сигналов (waveforms).   
3) Модель диффузии, которая отображает естественный язык в скрытые представления с помощью модели CLAP (contrastive language-audio pretraining). 
4) Отдельно обученный нейронный вокодер (vocoder) для преобразования мел-спектрограмм в необработанные (сырые) сигналы (waveforms).  

В данной модели для извлечения скрытого представления аудио и текста используется модель CLAP.  
Основная идея CLAP заключается в том, чтобы предварительно обучить модель на мультимодальных данных, включающих аудио и текст, используя метод контрастивного обучения.  
CLAP преобразовывает аудио и текст в общее пространство, где семантически похожие звуки и тексты находятся близко друг к другу. 

Чтобы уменьшить нехватку данных, был предложен алгоритм улучшения псевдоподсказок, который открывает возможность использования аудиофайлов без языкового содержания. 
Если коротко, то предобученные модели автоматических субтитров и поиска аудиотекста помогают создавать подсказки для описания содержания аудиоклипов. Затем используется техника динамического перепрограммирования, которая создает разнообразные композиции через три этапа: подготовка базы данных событий, выбор N композиций и объединение пар текст-аудио для создания новых обучающих примеров. Таким образом, метод улучшения псевдоподсказок помогает преодолеть проблемы нехватки данных. 

Модель была доработана под требования проекта: 
1) Стало возможно генерировать аудиозаписи произвольной продолжительности. 
2) Был добавлен код для преобразования выходного формата аудио в формат WAV. 
3) Была реализована обертка, которая максимально простым образом получает текстовый запрос и продолжительность, генерирует на основе них, далее формирует WAV файл и потом отправляет его в бота. 
4) Была оптимизирована скорость генерации аудиозаписи.
 
**Авторы: Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao.*
